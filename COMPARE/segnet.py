import os
import numpy as np
# from skimage.io import imread # æ ¹æ®æ–°çš„Datasetï¼Œå¯èƒ½ä¸å†éœ€è¦
import pandas as pd # ç”¨äºè¯»å–xlsxå’Œcsv
import cv2          # ç”¨äºresize
# from torchvision import transforms # ä¸å†éœ€è¦
from torch.utils.data import DataLoader, Dataset
# from torchvision.models.segmentation import fcn_resnet50 # æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„SegNet
import torch
from datetime import datetime
import csv
import torch.nn as nn
from tqdm import tqdm
from natsort import natsorted # ç”¨äºè‡ªç„¶æ’åº
import torch.nn.functional as F
import matplotlib.pyplot as plt
# from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, precision_score, f1_score # è¿™äº›æ˜¯åˆ†ç±»æŒ‡æ ‡
import time
from sklearn.model_selection import train_test_split

# --- 1. è®¾ç½®æ–‡ä»¶è·¯å¾„ ---
# è®¾ç½®åŸºç¡€è·¯å¾„
base_folder = r'E:\EMTdata'

# Eæ–‡ä»¶å¤¹ä¸‹çš„è·¯å¾„ (è¾“å…¥)
E_folder = os.path.join(base_folder, 'E')
if not os.path.exists(E_folder): # å¦‚æœ E å­æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™å‡å®š Re, Im, F ç›´æ¥åœ¨ base_folder ä¸‹
    print(f"è­¦å‘Š: è·¯å¾„ {E_folder} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ç›´æ¥åœ¨ {base_folder} ä¸‹å¯»æ‰¾ Re, Im, F æ–‡ä»¶å¤¹ã€‚")
    f_folder = os.path.join(base_folder, 'F')
    re_folder = os.path.join(base_folder, 'Re')
    im_folder = os.path.join(base_folder, 'Im')
else:
    f_folder = os.path.join(E_folder, 'F')
    re_folder = os.path.join(E_folder, 'Re')
    im_folder = os.path.join(E_folder, 'Im')


# labelæ–‡ä»¶å¤¹ä¸‹çš„è·¯å¾„ (æ ‡ç­¾)
label_folder = os.path.join(base_folder, 'label')
if not os.path.exists(label_folder): # å¦‚æœ label å­æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™å‡å®š label_Re, label_Im ç›´æ¥åœ¨ base_folder ä¸‹
    print(f"è­¦å‘Š: è·¯å¾„ {label_folder} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ç›´æ¥åœ¨ {base_folder} ä¸‹å¯»æ‰¾ label_Re, label_Im æ–‡ä»¶å¤¹ã€‚")
    label_re_folder = os.path.join(base_folder, 'label_Re')
    label_im_folder = os.path.join(base_folder, 'label_Im')
else:
    label_re_folder = os.path.join(label_folder, 'label_Re')
    label_im_folder = os.path.join(label_folder, 'label_Im')

print(f"è¾“å…¥ Re æ–‡ä»¶å¤¹: {re_folder}")
print(f"è¾“å…¥ Im æ–‡ä»¶å¤¹: {im_folder}")
print(f"è¾“å…¥ F æ–‡ä»¶å¤¹: {f_folder}")
print(f"æ ‡ç­¾ Re æ–‡ä»¶å¤¹: {label_re_folder}")
print(f"æ ‡ç­¾ Im æ–‡ä»¶å¤¹: {label_im_folder}")


# æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
path_tuples = [
    ("è¾“å…¥Re", re_folder, '.xlsx'), ("è¾“å…¥Im", im_folder, '.xlsx'), ("è¾“å…¥F", f_folder, '.xlsx'),
    ("æ ‡ç­¾Re", label_re_folder, '.csv'), ("æ ‡ç­¾Im", label_im_folder, '.csv')
]
all_paths_valid = True
for name, path, ext in path_tuples:
    if not os.path.isdir(path):
        print(f"é”™è¯¯: æ–‡ä»¶å¤¹ '{name}' åœ¨è·¯å¾„ '{path}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„è®¾ç½®ã€‚")
        all_paths_valid = False
if not all_paths_valid:
    exit("ç¨‹åºå› è·¯å¾„é”™è¯¯è€Œç»ˆæ­¢ã€‚")


re_files = natsorted([f for f in os.listdir(re_folder) if f.lower().endswith('.xlsx')])
im_files = natsorted([f for f in os.listdir(im_folder) if f.lower().endswith('.xlsx')])
f_files = natsorted([f for f in os.listdir(f_folder) if f.lower().endswith('.xlsx')])
label_re_files = natsorted([f for f in os.listdir(label_re_folder) if f.lower().endswith('.csv')])
label_im_files = natsorted([f for f in os.listdir(label_im_folder) if f.lower().endswith('.csv')])

min_len = min(len(re_files), len(im_files), len(f_files), len(label_re_files), len(label_im_files))
if min_len == 0:
    raise ValueError(f"ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®/æ ‡ç­¾æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œæˆ–ä¸åŒ…å«é¢„æœŸçš„ .xlsx æˆ– .csv æ–‡ä»¶ã€‚è¯·æ£€æŸ¥æ–‡ä»¶ï¼š\n"
                     f"Re (.xlsx): {len(re_files)} ä¸ªæ–‡ä»¶\n"
                     f"Im (.xlsx): {len(im_files)} ä¸ªæ–‡ä»¶\n"
                     f"F (.xlsx): {len(f_files)} ä¸ªæ–‡ä»¶\n"
                     f"label_Re (.csv): {len(label_re_files)} ä¸ªæ–‡ä»¶\n"
                     f"label_Im (.csv): {len(label_im_files)} ä¸ªæ–‡ä»¶")

re_files = re_files[:min_len]
im_files = im_files[:min_len]
f_files = f_files[:min_len]
label_re_files = label_re_files[:min_len]
label_im_files = label_im_files[:min_len]

(re_train_files, re_test_files,
 im_train_files, im_test_files,
 f_train_files, f_test_files,
 label_re_train_files, label_re_test_files,
 label_im_train_files, label_im_test_files) = train_test_split(
    re_files, im_files, f_files, label_re_files, label_im_files,
    test_size=0.2, random_state=42
)

# --- 2. æ•°æ®é›†å®šä¹‰ ---
# é¢‘ç‡é€šé“å½’ä¸€åŒ–å‡½æ•° (æ ¹æ®æ‚¨çš„å‚è€ƒè„šæœ¬)
def normalize_f_data(data, min_val=2.0, max_val=8.0):
    """å°†é¢‘ç‡æ•°æ®å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ (å‡è®¾åŸå§‹å€¼åœ¨[min_val, max_val]ä¹‹é—´)"""
    # ç¡®ä¿dataæ˜¯æµ®ç‚¹æ•°ä»¥è¿›è¡Œè®¡ç®—
    data = data.astype(np.float32)
    # é˜²æ­¢é™¤ä»¥é›¶
    if (max_val - min_val) == 0:
        return data - min_val # æˆ–è€…è¿”å›å…¨é›¶ï¼Œå–å†³äºå…·ä½“æœŸæœ›
    normalized_data = (data - min_val) / (max_val - min_val)
    return normalized_data

class MatrixDataset(Dataset): # é‡‡ç”¨æ‚¨æä¾›çš„ MatrixDataset
    def __init__(self, re_paths_list, im_paths_list, f_paths_list, label_re_paths_list, label_im_paths_list):
        self.re_paths = [os.path.join(re_folder, f) for f in re_paths_list] # æ„å»ºå®Œæ•´è·¯å¾„
        self.im_paths = [os.path.join(im_folder, f) for f in im_paths_list]
        self.f_paths = [os.path.join(f_folder, f) for f in f_paths_list]
        self.label_re_paths = [os.path.join(label_re_folder, f) for f in label_re_paths_list]
        self.label_im_paths = [os.path.join(label_im_folder, f) for f in label_im_paths_list]
        self.target_size = (512, 512)

    def __len__(self):
        return len(self.re_paths)

    def __getitem__(self, idx):
        # è¯»å–æ•°æ®
        try:
            re_data = pd.read_excel(self.re_paths[idx], header=0, engine='openpyxl').values
            im_data = pd.read_excel(self.im_paths[idx], header=0, engine='openpyxl').values
            f_data_orig = pd.read_excel(self.f_paths[idx], header=0, engine='openpyxl').values
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶ (xlsx) æ—¶å‡ºé”™: index {idx}")
            print(f"Re path: {self.re_paths[idx]}")
            print(f"Im path: {self.im_paths[idx]}")
            print(f"F path: {self.f_paths[idx]}")
            print(f"é”™è¯¯: {e}")
            raise

        f_data_normalized = normalize_f_data(f_data_orig) # å½’ä¸€åŒ–Fé€šé“

        # resize åˆ° (512, 512)
        # pandasè¯»å–çš„.valueså¯èƒ½æ˜¯objectç±»å‹æˆ–æ•´æ•°ï¼Œcv2.resizeéœ€è¦float32æˆ–uint8
        re_data_resized = cv2.resize(re_data.astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)
        im_data_resized = cv2.resize(im_data.astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)
        f_data_resized = cv2.resize(f_data_normalized.astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)

        # å †å è¾“å…¥æ•°æ® (C, H, W) -> (3, 512, 512)
        input_data_np = np.stack([re_data_resized, im_data_resized, f_data_resized], axis=0)
        input_tensor = torch.from_numpy(input_data_np).float()

        # è¯»å–å¹¶ resize æ ‡ç­¾
        try:
            label_re_orig = pd.read_csv(self.label_re_paths[idx], header=None).values
            label_im_orig = pd.read_csv(self.label_im_paths[idx], header=None).values
        except Exception as e:
            print(f"è¯»å–æ ‡ç­¾æ–‡ä»¶ (csv) æ—¶å‡ºé”™: index {idx}")
            print(f"Label Re path: {self.label_re_paths[idx]}")
            print(f"Label Im path: {self.label_im_paths[idx]}")
            print(f"é”™è¯¯: {e}")
            raise

        label_re_resized = cv2.resize(label_re_orig.astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)
        label_im_resized = cv2.resize(label_im_orig.astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)

        # å †å æ ‡ç­¾æ•°æ® (C, H, W) -> (2, 512, 512)
        label_data_np = np.stack([label_re_resized, label_im_resized], axis=0)
        label_tensor = torch.from_numpy(label_data_np).float()

        return {'image': input_tensor, 'label': label_tensor}

# --- 3. SegNet æ¨¡å‹å®šä¹‰ ---
class SegNet(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(SegNet, self).__init__()
        self.enc1 = self.conv_block(input_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)

        self.dec4 = self.upconv_block(512, 256)
        self.dec3 = self.upconv_block(256, 128)
        self.dec2 = self.upconv_block(128, 64)
        self.final_dec_layer = nn.ConvTranspose2d(64, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)

    def forward(self, x): # x é¢„æœŸå°ºå¯¸ (B, C_in, 512, 512)
        original_size = x.shape[2:]

        x1_conv = self.enc1(x)
        x1_pool, idx1 = F.max_pool2d(x1_conv, kernel_size=2, stride=2, return_indices=True)
        x2_conv = self.enc2(x1_pool)
        x2_pool, idx2 = F.max_pool2d(x2_conv, kernel_size=2, stride=2, return_indices=True)
        x3_conv = self.enc3(x2_pool)
        x3_pool, idx3 = F.max_pool2d(x3_conv, kernel_size=2, stride=2, return_indices=True)
        x4_conv = self.enc4(x3_pool)
        x4_pool, idx4 = F.max_pool2d(x4_conv, kernel_size=2, stride=2, return_indices=True)

        d4_unpool = F.max_unpool2d(x4_pool, idx4, kernel_size=2, stride=2, output_size=x4_conv.size())
        d4 = self.dec4(d4_unpool)
        d3_unpool = F.max_unpool2d(d4, idx3, kernel_size=2, stride=2, output_size=x3_conv.size())
        d3 = self.dec3(d3_unpool)
        d2_unpool = F.max_unpool2d(d3, idx2, kernel_size=2, stride=2, output_size=x2_conv.size())
        d2 = self.dec2(d2_unpool)
        out_unpool_final = F.max_unpool2d(d2, idx1, kernel_size=2, stride=2, output_size=x1_conv.size())
        out = self.final_dec_layer(out_unpool_final)

        if out.shape[2:] != original_size: # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸å›ºå®šè¾“å…¥å°ºå¯¸(512,512)ä¸€è‡´
            out = F.interpolate(out, size=original_size, mode='bilinear', align_corners=True)
        return out

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def upconv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

# --- 4. è®­ç»ƒé€»è¾‘ ---
# æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿®æ­£ç‰ˆ)
def calculate_regression_metrics(outputs, labels):
    mse = F.mse_loss(outputs, labels).item()
    # RMSE æ˜¯ MSE çš„å¹³æ–¹æ ¹
    rmse = torch.sqrt(torch.tensor(mse)).item() # æˆ–è€…ç›´æ¥ math.sqrt(mse)
    return mse, rmse

if __name__ == '__main__':
    # å®šä¹‰æ¨¡å‹å’Œæ—¥å¿—çš„ä¿å­˜è·¯å¾„ (ä½¿ç”¨ "SegNet" æ ‡è¯†)
    checkpoint_dir = r"./SegNet_XLSX_Output_final" # å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹æ­¤è¾“å‡ºç›®å½•
    os.makedirs(checkpoint_dir, exist_ok=True)
    latest_model_path = os.path.join(checkpoint_dir, "segnet_latest_model.pth")
    best_model_path = os.path.join(checkpoint_dir, "segnet_best_model.pth")
    log_path = os.path.join(checkpoint_dir, "segnet_training_log.csv")

    # æ•°æ®é›†å®ä¾‹åŒ–
    train_dataset = MatrixDataset(
        re_paths_list=re_train_files, im_paths_list=im_train_files, f_paths_list=f_train_files,
        label_re_paths_list=label_re_train_files, label_im_paths_list=label_im_train_files
    )
    # batch_size å’Œ num_workers æ ¹æ®æ‚¨çš„ç¡¬ä»¶è¿›è¡Œè°ƒæ•´
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)

    if re_test_files: # ä»…å½“å­˜åœ¨æµ‹è¯•æ–‡ä»¶æ—¶åˆ›å»ºæµ‹è¯•åŠ è½½å™¨
        test_dataset = MatrixDataset(
            re_paths_list=re_test_files, im_paths_list=im_test_files, f_paths_list=f_test_files,
            label_re_paths_list=label_re_test_files, label_im_paths_list=label_im_test_files
        )
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)
    else:
        test_loader = None
        print("è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ (re_test_files ä¸ºç©º)ï¼Œå°†è·³è¿‡éªŒè¯è¿‡ç¨‹ã€‚")

    # æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SegNet(input_channels=3, output_channels=2).to(device) # 3é€šé“è¾“å…¥, 2é€šé“è¾“å‡º
    criterion = nn.MSELoss() # å‡æ–¹è¯¯å·®æŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # å­¦ä¹ ç‡

    # è®­ç»ƒçŠ¶æ€å˜é‡
    start_epoch = 0
    best_val_loss = float('inf')
    train_losses_history, val_losses_history = [], [] # ä½¿ç”¨æ›´æ˜ç¡®çš„å˜é‡å
    train_mses_history, train_rmses_history = [], []
    val_mses_history, val_rmses_history = [], []

    # sshæ–­è”äº†ï¼åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(latest_model_path):
        print(f"ğŸŸ¡ æ­£åœ¨ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹æ¢å¤: {latest_model_path}")
        checkpoint = torch.load(latest_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', -1) + 1 # .get æä¾›é»˜è®¤å€¼
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            train_losses_history = checkpoint.get('train_losses_history', [])
            val_losses_history = checkpoint.get('val_losses_history', [])
            train_mses_history = checkpoint.get('train_mses_history', [])
            train_rmses_history = checkpoint.get('train_rmses_history', [])
            val_mses_history = checkpoint.get('val_mses_history', [])
            val_rmses_history = checkpoint.get('val_rmses_history', [])
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚å°†ä» epoch {start_epoch} å¼€å§‹ç»§ç»­è®­ç»ƒã€‚å½“å‰æœ€ä½³éªŒè¯Loss: {best_val_loss:.6f}")
        except Exception as e: # æ›´é€šç”¨çš„å¼‚å¸¸æ•è·
            print(f"è­¦å‘Šï¼šåŠ è½½ä¼˜åŒ–å™¨æˆ–å†å²æŒ‡æ ‡æ—¶å‡ºé”™ ({e})ã€‚éƒ¨åˆ†çŠ¶æ€å¯èƒ½æœªæ¢å¤ã€‚")
            # å¯ä»¥é€‰æ‹©é‡ç½®è¿™äº›çŠ¶æ€æˆ–å°è¯•éƒ¨åˆ†æ¢å¤
            start_epoch = checkpoint.get('epoch', -1) + 1
            best_val_loss = checkpoint.get('best_val_loss', float('inf')) # è‡³å°‘å°è¯•æ¢å¤è¿™ä¸ª
    else:
        print("âšª æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        if not os.path.exists(log_path):
            with open(log_path, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['epoch', 'train_loss', 'train_mse', 'train_rmse', 'val_loss', 'val_mse', 'val_rmse'])

    print(f'è®­ç»ƒåˆå§‹åŒ–å®Œæˆã€‚è®¾å¤‡: {device}')
    num_epochs = 100 # è®­ç»ƒè½®æ¬¡ (æ ¹æ®éœ€è¦è°ƒæ•´)

    for epoch in range(start_epoch, num_epochs):
        model.train()
        epoch_train_loss = 0.0
        epoch_train_mse = 0.0
        epoch_train_rmse = 0.0

        progress_bar_train = tqdm(train_loader,
                                  desc=f'Epoch {epoch+1}/{num_epochs} [è®­ç»ƒ]',
                                  ncols=120,     # è¿›åº¦æ¡å®½åº¦ï¼Œå¯è°ƒæ•´
                                  miniters=10,   # <-- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ¯å¤„ç†10ä¸ªæ‰¹æ¬¡ï¼ˆè¿­ä»£ï¼‰æ‰åˆ·æ–°ä¸€æ¬¡æ˜¾ç¤º
                                  mininterval=0, # <-- ç¡®ä¿æ›´æ–°ä¸»è¦ç”± miniters æ§åˆ¶ (0è¡¨ç¤ºå°½å¯èƒ½å¿«ï¼Œä½†ä¼šè¢«minitersé™åˆ¶)
                                  leave=True)    # å¾ªç¯ç»“æŸåä¿ç•™è¿›åº¦æ¡çŠ¶æ€
        for batch_idx,batch_data in enumerate(progress_bar_train):
            inputs = batch_data['image'].to(device)
            labels = batch_data['label'].to(device) # ä½¿ç”¨ 'label' é”®

            optimizer.zero_grad()
            outputs = model(inputs) # SegNetç›´æ¥è¾“å‡ºå¼ é‡
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            mse, rmse = calculate_regression_metrics(outputs, labels)
            epoch_train_loss += loss.item()
            epoch_train_mse += mse
            epoch_train_rmse += rmse

            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader): # <--- ä¿®æ”¹2ï¼šæ·»åŠ  if æ¡ä»¶
                progress_bar_train.set_postfix(loss=loss.item(), mse=mse, rmse=rmse)


        avg_epoch_train_loss = epoch_train_loss / len(train_loader) if len(train_loader) > 0 else 0
        avg_epoch_train_mse = epoch_train_mse / len(train_loader) if len(train_loader) > 0 else 0
        avg_epoch_train_rmse = epoch_train_rmse / len(train_loader) if len(train_loader) > 0 else 0

        train_losses_history.append(avg_epoch_train_loss)
        train_mses_history.append(avg_epoch_train_mse)
        train_rmses_history.append(avg_epoch_train_rmse)
        print(f"Epoch {epoch+1} è®­ç»ƒæ€»ç»“: Loss: {avg_epoch_train_loss:.4f}, MSE: {avg_epoch_train_mse:.4f}, RMSE: {avg_epoch_train_rmse:.4f}")

        # éªŒè¯
        avg_epoch_val_loss = None
        if test_loader:
            model.eval()
            epoch_val_loss = 0.0
            epoch_val_mse = 0.0
            epoch_val_rmse = 0.0
            with torch.no_grad():
                progress_bar_val = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [éªŒè¯]', ncols=110)
                for batch_data in progress_bar_val:
                    inputs = batch_data['image'].to(device)
                    labels = batch_data['label'].to(device)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    mse, rmse = calculate_regression_metrics(outputs, labels)
                    epoch_val_loss += loss.item()
                    epoch_val_mse += mse
                    epoch_val_rmse += rmse
                    progress_bar_val.set_postfix(loss=loss.item(), mse=mse, rmse=rmse)

            avg_epoch_val_loss = epoch_val_loss / len(test_loader) if len(test_loader) > 0 else 0
            avg_epoch_val_mse = epoch_val_mse / len(test_loader) if len(test_loader) > 0 else 0
            avg_epoch_val_rmse = epoch_val_rmse / len(test_loader) if len(test_loader) > 0 else 0

            val_losses_history.append(avg_epoch_val_loss)
            val_mses_history.append(avg_epoch_val_mse)
            val_rmses_history.append(avg_epoch_val_rmse)
            print(f"Epoch {epoch+1} éªŒè¯æ€»ç»“: Loss: {avg_epoch_val_loss:.4f}, MSE: {avg_epoch_val_mse:.4f}, RMSE: {avg_epoch_val_rmse:.4f}")

            if avg_epoch_val_loss < best_val_loss:
                best_val_loss = avg_epoch_val_loss
                torch.save({
                    'epoch': epoch, 'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss,
                    'train_losses_history': train_losses_history, 'val_losses_history': val_losses_history,
                    'train_mses_history': train_mses_history, 'train_rmses_history': train_rmses_history,
                    'val_mses_history': val_mses_history, 'val_rmses_history': val_rmses_history
                }, best_model_path)
                print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {best_model_path}ï¼ŒéªŒè¯ Loss: {best_val_loss:.6f}")
        else: # å¦‚æœæ²¡æœ‰éªŒè¯åŠ è½½å™¨
            val_losses_history.append(float('nan')) # ä½¿ç”¨ NaN ä½œä¸ºå ä½ç¬¦
            val_mses_history.append(float('nan'))
            val_rmses_history.append(float('nan'))


        # ä¿å­˜æœ€æ–°æ¨¡å‹
        torch.save({
            'epoch': epoch, 'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss,
            'train_losses_history': train_losses_history, 'val_losses_history': val_losses_history,
            'train_mses_history': train_mses_history, 'train_rmses_history': train_rmses_history,
            'val_mses_history': val_mses_history, 'val_rmses_history': val_rmses_history
        }, latest_model_path)
        # print(f"â˜‘ï¸ æœ€æ–°æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {latest_model_path}") # å‡å°‘æ‰“å°é¢‘ç‡

        # è®°å½•æ—¥å¿—
        with open(log_path, 'a', newline='') as f:
            writer = csv.writer(f)
            val_loss_log = f"{avg_epoch_val_loss:.4f}" if avg_epoch_val_loss is not None else "N/A"
            val_mse_log = f"{avg_epoch_val_mse:.4f}" if avg_epoch_val_loss is not None else "N/A" # åŸºäºavg_epoch_val_lossåˆ¤æ–­æ˜¯å¦æœ‰éªŒè¯
            val_rmse_log = f"{avg_epoch_val_rmse:.4f}" if avg_epoch_val_loss is not None else "N/A"
            writer.writerow([epoch + 1, avg_epoch_train_loss, avg_epoch_train_mse, avg_epoch_train_rmse,
                             val_loss_log, val_mse_log, val_rmse_log])
    print('ğŸ è®­ç»ƒå®Œæˆã€‚')

    # ç»˜åˆ¶å›¾è¡¨
    plt.figure(figsize=(15, 5))
    plt.subplot(1, 3, 1)
    plt.plot(train_losses_history, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses_history, label='éªŒè¯æŸå¤±', linestyle='--')
    plt.xlabel('è½®æ¬¡ (Epoch)'); plt.ylabel('æŸå¤± (Loss)'); plt.legend(); plt.title('æŸå¤±å˜åŒ–')
    plt.grid(True)

    plt.subplot(1, 3, 2)
    plt.plot(train_mses_history, label='è®­ç»ƒ MSE')
    plt.plot(val_mses_history, label='éªŒè¯ MSE', linestyle='--')
    plt.xlabel('è½®æ¬¡ (Epoch)'); plt.ylabel('MSE'); plt.legend(); plt.title('å‡æ–¹è¯¯å·® (MSE)')
    plt.grid(True)

    plt.subplot(1, 3, 3)
    plt.plot(train_rmses_history, label='è®­ç»ƒ RMSE')
    plt.plot(val_rmses_history, label='éªŒè¯ RMSE', linestyle='--')
    plt.xlabel('è½®æ¬¡ (Epoch)'); plt.ylabel('RMSE'); plt.legend(); plt.title('å‡æ–¹æ ¹è¯¯å·® (RMSE)')
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(os.path.join(checkpoint_dir, "training_metrics_segnet_xlsx_zh.png"))
    print(f"ğŸ“ˆ è®­ç»ƒæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜åˆ° {os.path.join(checkpoint_dir, 'training_metrics_segnet_xlsx_zh.png')}")