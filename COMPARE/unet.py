import os
import numpy as np
import pandas as pd
import cv2
from torch.utils.data import DataLoader, Dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from natsort import natsorted
from tqdm import tqdm
from datetime import datetime
import csv
import time
import math # ç”¨äº math.sqrt

# --- 1. æ–‡ä»¶è·¯å¾„è®¾ç½® (ä¿æŒä¸å˜) ---
# base_folder = r'/root/EMTdata'
base_folder = r'E:\EMTdata'

E_folder = os.path.join(base_folder, 'E')
if not os.path.exists(E_folder):
    print(f"è­¦å‘Š: è·¯å¾„ {E_folder} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ç›´æ¥åœ¨ {base_folder} ä¸‹å¯»æ‰¾ Re, Im, F æ–‡ä»¶å¤¹ã€‚")
    f_folder_path = os.path.join(base_folder, 'F')
    re_folder_path = os.path.join(base_folder, 'Re')
    im_folder_path = os.path.join(base_folder, 'Im')
else:
    f_folder_path = os.path.join(E_folder, 'F')
    re_folder_path = os.path.join(E_folder, 'Re')
    im_folder_path = os.path.join(E_folder, 'Im')

label_folder = os.path.join(base_folder, 'label')
if not os.path.exists(label_folder):
    print(f"è­¦å‘Š: è·¯å¾„ {label_folder} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ç›´æ¥åœ¨ {base_folder} ä¸‹å¯»æ‰¾ label_Re, label_Im æ–‡ä»¶å¤¹ã€‚")
    label_re_folder_path = os.path.join(base_folder, 'label_Re')
    label_im_folder_path = os.path.join(base_folder, 'label_Im')
else:
    label_re_folder_path = os.path.join(label_folder, 'label_Re')
    label_im_folder_path = os.path.join(label_folder, 'label_Im')

print(f"è¾“å…¥ Re æ–‡ä»¶å¤¹: {re_folder_path}")
print(f"è¾“å…¥ Im æ–‡ä»¶å¤¹: {im_folder_path}")
print(f"è¾“å…¥ F æ–‡ä»¶å¤¹: {f_folder_path}")
print(f"æ ‡ç­¾ Re æ–‡ä»¶å¤¹: {label_re_folder_path}")
print(f"æ ‡ç­¾ Im æ–‡ä»¶å¤¹: {label_im_folder_path}")


path_tuples = [
    ("è¾“å…¥Re", re_folder_path, '.xlsx'), ("è¾“å…¥Im", im_folder_path, '.xlsx'), ("è¾“å…¥F", f_folder_path, '.xlsx'),
    ("æ ‡ç­¾Re", label_re_folder_path, '.csv'), ("æ ‡ç­¾Im", label_im_folder_path, '.csv')
]
all_paths_valid = True
for name, path, ext in path_tuples:
    if not os.path.isdir(path):
        print(f"é”™è¯¯: æ–‡ä»¶å¤¹ '{name}' åœ¨è·¯å¾„ '{path}' æœªæ‰¾åˆ°ã€‚")
        all_paths_valid = False
if not all_paths_valid: exit("ç¨‹åºå› è·¯å¾„é”™è¯¯è€Œç»ˆæ­¢ã€‚")

re_files_list = natsorted([f for f in os.listdir(re_folder_path) if f.lower().endswith('.xlsx')])
im_files_list = natsorted([f for f in os.listdir(im_folder_path) if f.lower().endswith('.xlsx')])
f_files_list = natsorted([f for f in os.listdir(f_folder_path) if f.lower().endswith('.xlsx')])
label_re_files_list = natsorted([f for f in os.listdir(label_re_folder_path) if f.lower().endswith('.csv')])
label_im_files_list = natsorted([f for f in os.listdir(label_im_folder_path) if f.lower().endswith('.csv')])

min_len = min(len(re_files_list), len(im_files_list), len(f_files_list), len(label_re_files_list), len(label_im_files_list))
if min_len == 0: raise ValueError(f"ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®/æ ‡ç­¾æ–‡ä»¶å¤¹ä¸ºç©ºã€‚")

re_files_list, im_files_list, f_files_list, label_re_files_list, label_im_files_list = \
    [lst[:min_len] for lst in [re_files_list, im_files_list, f_files_list, label_re_files_list, label_im_files_list]]

(re_train_files, re_test_files, im_train_files, im_test_files, f_train_files, f_test_files,
 label_re_train_files, label_re_test_files, label_im_train_files, label_im_test_files) = train_test_split(
    re_files_list, im_files_list, f_files_list, label_re_files_list, label_im_files_list,
    test_size=0.2, random_state=42
)

# --- 2. æ•°æ®é›†å®šä¹‰ (ä¿®æ”¹å) ---
def normalize_f_data(data, min_val=2.0, max_val=8.0):
    data = data.astype(np.float32)
    if (max_val - min_val) == 0: return data - min_val
    return (data - min_val) / (max_val - min_val)

class MatrixDataset(Dataset):
    def __init__(self, re_fnames, im_fnames, f_fnames, label_re_fnames, label_im_fnames,
                 re_mean=None, re_std=None, im_mean=None, im_std=None, calculate_stats_mode=False): # æ–°å¢ç»Ÿè®¡å‚æ•°å’Œæ¨¡å¼
        self.re_paths = [os.path.join(re_folder_path, f) for f in re_fnames]
        self.im_paths = [os.path.join(im_folder_path, f) for f in im_fnames]
        self.f_paths = [os.path.join(f_folder_path, f) for f in f_fnames]
        self.label_re_paths = [os.path.join(label_re_folder_path, f) for f in label_re_fnames]
        self.label_im_paths = [os.path.join(label_im_folder_path, f) for f in label_im_fnames]
        self.target_size = (512, 512)

        self.re_mean = re_mean
        self.re_std = re_std
        self.im_mean = im_mean
        self.im_std = im_std
        self.calculate_stats_mode = calculate_stats_mode # ç”¨äºç»Ÿè®¡è®¡ç®—æ—¶è¿”å›åŸå§‹Re/Imæ•°æ®

    def __len__(self):
        return len(self.re_paths)

    def __getitem__(self, idx):
        re_data = pd.read_excel(self.re_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        im_data = pd.read_excel(self.im_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        f_data_orig = pd.read_excel(self.f_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        
        f_data_normalized = normalize_f_data(f_data_orig)

        re_data_resized = cv2.resize(re_data, self.target_size, interpolation=cv2.INTER_LINEAR)
        im_data_resized = cv2.resize(im_data, self.target_size, interpolation=cv2.INTER_LINEAR)
        f_data_resized = cv2.resize(f_data_normalized, self.target_size, interpolation=cv2.INTER_LINEAR)

        if not self.calculate_stats_mode: # æ­£å¸¸æ¨¡å¼ä¸‹è¿›è¡Œå½’ä¸€åŒ–
            if self.re_mean is not None and self.re_std is not None:
                re_data_resized = (re_data_resized - self.re_mean) / self.re_std
            if self.im_mean is not None and self.im_std is not None:
                im_data_resized = (im_data_resized - self.im_mean) / self.im_std
        
        input_data_np = np.stack([re_data_resized, im_data_resized, f_data_resized], axis=0)
        input_tensor = torch.from_numpy(input_data_np).float()

        if self.calculate_stats_mode: # å¦‚æœåªæ˜¯è®¡ç®—ç»Ÿè®¡é‡ï¼Œä¸éœ€è¦æ ‡ç­¾
            return {'image': input_tensor} # æˆ–è€…åªè¿”å›éœ€è¦ç»Ÿè®¡çš„é€šé“

        label_re_orig = pd.read_csv(self.label_re_paths[idx], header=None).values.astype(np.float32)
        label_im_orig = pd.read_csv(self.label_im_paths[idx], header=None).values.astype(np.float32)
        label_re_resized = cv2.resize(label_re_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_im_resized = cv2.resize(label_im_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_data_np = np.stack([label_re_resized, label_im_resized], axis=0)
        label_tensor = torch.from_numpy(label_data_np).float()

        return {'image': input_tensor, 'label': label_tensor}

# --- 3. PyTorch U-Net æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜) ---
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__(); mid_channels = mid_channels or out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False), nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False), nn.ReLU(inplace=True))
    def forward(self, x): return self.double_conv(x)

class Down(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__(); self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))
    def forward(self, x): return self.maxpool_conv(x)

class Up(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=False):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        diffY = x2.size()[2] - x1.size()[2]; diffX = x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1); return self.conv(x)

class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__(); self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
    def forward(self, x): return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels_in, n_channels_out, bilinear=False):
        super(UNet, self).__init__()
        self.inc = DoubleConv(n_channels_in, 64)
        self.down1 = Down(64, 128); self.down2 = Down(128, 256); self.down3 = Down(256, 512)
        self.dropout_enc = nn.Dropout(0.5)
        factor = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        self.dropout_bottleneck = nn.Dropout(0.5)
        self.up1 = Up(1024, 512 // factor, bilinear); self.up2 = Up(512, 256 // factor, bilinear)
        self.up3 = Up(256, 128 // factor, bilinear); self.up4 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_channels_out)
    def forward(self, x):
        x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2)
        x4_before_dropout = self.down3(x3); x4 = self.dropout_enc(x4_before_dropout)
        x5_before_dropout = self.down4(x4); x5 = self.dropout_bottleneck(x5_before_dropout)
        out = self.up1(x5, x4); out = self.up2(out, x3); out = self.up3(out, x2)
        out = self.up4(out, x1); logits = self.outc(out)
        if logits.shape[2:] != x.shape[2:]: # ç¡®ä¿å°ºå¯¸æœ€ç»ˆä¸€è‡´
            logits = F.interpolate(logits, size=x.shape[2:], mode='bilinear', align_corners=True)
        return logits

# --- 4. è®­ç»ƒé€»è¾‘ (ä¿®æ”¹å) ---
def calculate_regression_metrics(outputs, labels):
    mse = F.mse_loss(outputs, labels).item()
    rmse = math.sqrt(mse) # ä½¿ç”¨ math.sqrt
    return mse, rmse

if __name__ == '__main__':
    checkpoint_dir = r"./UNet_XLSX_Regression_Normalized_Output"
    os.makedirs(checkpoint_dir, exist_ok=True)
    latest_model_path = os.path.join(checkpoint_dir, "unet_latest_model_norm.pth")
    best_model_path = os.path.join(checkpoint_dir, "unet_best_model_norm.pth")
    log_path = os.path.join(checkpoint_dir, "unet_training_log_norm.csv")

    # --- é¢„è®¡ç®—è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·® ---
    print("âœ¨ æ­£åœ¨è®¡ç®—è®­ç»ƒé›†Reå’ŒImé€šé“çš„å‡å€¼å’Œæ ‡å‡†å·®...")
    stat_dataset = MatrixDataset( # ä¸ä¼ å…¥å‡å€¼/æ ‡å‡†å·®ï¼Œæˆ–è®¾ç½® calculate_stats_mode=True
        re_fnames=re_train_files, im_fnames=im_train_files, f_fnames=f_train_files,
        label_re_fnames=label_re_train_files, label_im_fnames=label_im_train_files, # æ ‡ç­¾è·¯å¾„ä»éœ€æä¾›ï¼Œå³ä½¿ä¸ç”¨äºç»Ÿè®¡è¾“å…¥
        calculate_stats_mode=True # å‘ŠçŸ¥Datasetåªä¸ºç»Ÿè®¡å‡†å¤‡æ•°æ®
    )
    stat_loader = DataLoader(stat_dataset, batch_size=16, shuffle=False, num_workers=0)

    re_sum, im_sum = 0.0, 0.0
    re_sum_sq, im_sum_sq = 0.0, 0.0
    total_pixels_re, total_pixels_im = 0, 0

    for batch_data in tqdm(stat_loader, desc="è®¡ç®—ç»Ÿè®¡é‡"):
        re_channel_data = batch_data['image'][:, 0, :, :]
        im_channel_data = batch_data['image'][:, 1, :, :]
        re_sum += torch.sum(re_channel_data).item()
        im_sum += torch.sum(im_channel_data).item()
        re_sum_sq += torch.sum(torch.square(re_channel_data)).item()
        im_sum_sq += torch.sum(torch.square(im_channel_data)).item()
        total_pixels_re += re_channel_data.nelement()
        total_pixels_im += im_channel_data.nelement()
    
    if total_pixels_re == 0 or total_pixels_im == 0 :
        print("è­¦å‘Š: æœªèƒ½å¤„ç†ä»»ä½•åƒç´ æ¥è®¡ç®—Reæˆ–Imé€šé“çš„ç»Ÿè®¡æ•°æ®ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ (mean=0, std=1)ã€‚")
        re_mean_train, re_std_train = 0.0, 1.0
        im_mean_train, im_std_train = 0.0, 1.0
    else:
        re_mean_train = re_sum / total_pixels_re
        im_mean_train = im_sum / total_pixels_im
        re_std_train = math.sqrt(max(0, (re_sum_sq / total_pixels_re) - (re_mean_train**2)))
        im_std_train = math.sqrt(max(0, (im_sum_sq / total_pixels_im) - (im_mean_train**2)))
        re_std_train = max(re_std_train, 1e-7)
        im_std_train = max(im_std_train, 1e-7)

    print(f"è®¡ç®—å¾—åˆ°çš„è®­ç»ƒé›†ç»Ÿè®¡æ•°æ®:")
    print(f"  Re é€šé“: å‡å€¼={re_mean_train:.4f}, æ ‡å‡†å·®={re_std_train:.4f}")
    print(f"  Im é€šé“: å‡å€¼={im_mean_train:.4f}, æ ‡å‡†å·®={im_std_train:.4f}")
    print("--- ç»Ÿè®¡æ•°æ®è®¡ç®—å®Œæ¯• ---")

    train_dataset = MatrixDataset(
        re_fnames=re_train_files, im_fnames=im_train_files, f_fnames=f_train_files,
        label_re_fnames=label_re_train_files, label_im_fnames=label_im_train_files,
        re_mean=re_mean_train, re_std=re_std_train,
        im_mean=im_mean_train, im_std=im_std_train
    )
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)

    if re_test_files:
        test_dataset = MatrixDataset(
            re_fnames=re_test_files, im_fnames=im_test_files, f_fnames=f_test_files,
            label_re_fnames=label_re_test_files, label_im_fnames=label_im_test_files,
            re_mean=re_mean_train, re_std=re_std_train,
            im_mean=im_mean_train, im_std=im_std_train
        )
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)
    else:
        test_loader = None; print("è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ï¼Œè·³è¿‡éªŒè¯ã€‚æ—©åœç­–ç•¥å°†ä¸å¯ç”¨ã€‚") # MODIFIED: Added "æ—©åœç­–ç•¥å°†ä¸å¯ç”¨ã€‚"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = UNet(n_channels_in=3, n_channels_out=2).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    start_epoch = 0
    best_val_loss = float('inf')
    train_losses_history, val_losses_history = [], []
    train_mses_history, train_rmses_history = [], []
    val_mses_history, val_rmses_history = [], []

    if os.path.exists(latest_model_path):
        print(f"ğŸŸ¡ æ­£åœ¨ä»æœ€æ–°çš„U-Netæ£€æŸ¥ç‚¹æ¢å¤: {latest_model_path}")
        checkpoint = torch.load(latest_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', -1) + 1
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            train_losses_history = checkpoint.get('train_losses_history', [])
            val_losses_history = checkpoint.get('val_losses_history', [])
            train_mses_history = checkpoint.get('train_mses_history', [])
            train_rmses_history = checkpoint.get('train_rmses_history', [])
            val_mses_history = checkpoint.get('val_mses_history', [])
            val_rmses_history = checkpoint.get('val_rmses_history', [])
            # early_stopping_counter = checkpoint.get('early_stopping_counter', 0) # å¯é€‰ï¼šå¦‚æœä¹Ÿä¿å­˜äº†è®¡æ•°å™¨
            print(f"âœ… U-Netæ¨¡å‹åŠ è½½å®Œæ¯•ã€‚å°†ä» epoch {start_epoch} å¼€å§‹ã€‚æœ€ä½³éªŒè¯Loss: {best_val_loss:.6f}")
        except Exception as e:
            print(f"è­¦å‘Šï¼šåŠ è½½U-Netä¼˜åŒ–å™¨æˆ–å†å²æŒ‡æ ‡æ—¶å‡ºé”™ ({e})ã€‚")
            start_epoch = checkpoint.get('epoch', -1) + 1
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
    else:
        print("âšª æœªæ‰¾åˆ°U-Netæ£€æŸ¥ç‚¹ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        if not os.path.exists(log_path):
            with open(log_path, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['epoch', 'train_loss', 'train_mse', 'train_rmse', 'val_loss', 'val_mse', 'val_rmse'])

    print(f'U-Netè®­ç»ƒåˆå§‹åŒ–å®Œæˆã€‚è®¾å¤‡: {device}')
    num_epochs = 1000

    # --- æ—©åœå‚æ•°åˆå§‹åŒ– (MODIFIED) ---
    early_stopping_patience = 50  # æŒ‰è¦æ±‚è®¾ç½®ä¸º50
    early_stopping_counter = 0

    for epoch in range(start_epoch, num_epochs):
        model.train()
        epoch_train_loss, epoch_train_mse, epoch_train_rmse = 0.0, 0.0, 0.0
        progress_bar_train = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [U-Netè®­ç»ƒ]', ncols=120, miniters=10, mininterval=0, leave=False)
        
        for batch_idx, batch_data in enumerate(progress_bar_train):
            inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)
            optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)
            loss.backward(); optimizer.step()
            mse, rmse = calculate_regression_metrics(outputs, labels)
            epoch_train_loss += loss.item(); epoch_train_mse += mse; epoch_train_rmse += rmse
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):
                progress_bar_train.set_postfix(loss=loss.item(), mse=mse, rmse=rmse)

        len_train_loader = len(train_loader) if len(train_loader) > 0 else 1
        avg_epoch_train_loss = epoch_train_loss / len_train_loader
        avg_epoch_train_mse = epoch_train_mse / len_train_loader
        avg_epoch_train_rmse = epoch_train_rmse / len_train_loader
        train_losses_history.append(avg_epoch_train_loss); train_mses_history.append(avg_epoch_train_mse); train_rmses_history.append(avg_epoch_train_rmse)
        tqdm.write(f"Epoch {epoch+1} U-Netè®­ç»ƒæ€»ç»“: Loss={avg_epoch_train_loss:.4f}, MSE={avg_epoch_train_mse:.4f}, RMSE={avg_epoch_train_rmse:.4f}")

        avg_epoch_val_loss = float('nan') 
        avg_epoch_val_mse = float('nan')
        avg_epoch_val_rmse = float('nan')
        
        if test_loader:
            model.eval(); epoch_val_loss, epoch_val_mse, epoch_val_rmse = 0.0,0.0,0.0
            progress_bar_val = None # åˆå§‹åŒ– progress_bar_val
            with torch.no_grad():
                progress_bar_val = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [U-NetéªŒè¯]', ncols=120, leave=False)
                for batch_data in progress_bar_val:
                    inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)
                    outputs = model(inputs); loss = criterion(outputs, labels)
                    mse, rmse = calculate_regression_metrics(outputs, labels)
                    epoch_val_loss += loss.item(); epoch_val_mse += mse; epoch_val_rmse += rmse
                    progress_bar_val.set_postfix(loss=loss.item(), mse=mse, rmse=rmse)
            
            len_test_loader = len(test_loader) if len(test_loader) > 0 else 1
            avg_epoch_val_loss = epoch_val_loss / len_test_loader
            avg_epoch_val_mse = epoch_val_mse / len_test_loader
            avg_epoch_val_rmse = epoch_val_rmse / len_test_loader
            
            # --- å…³é”®: è®°å½•å®é™…çš„éªŒè¯æŒ‡æ ‡ (è¿™éƒ¨åˆ†åœ¨æ‚¨çš„ä»£ç ä¸­å·²ç»æ˜¯æ­£ç¡®çš„) ---
            val_losses_history.append(avg_epoch_val_loss); val_mses_history.append(avg_epoch_val_mse); val_rmses_history.append(avg_epoch_val_rmse)
            tqdm.write(f"Epoch {epoch+1} U-NetéªŒè¯æ€»ç»“: Loss={avg_epoch_val_loss:.4f}, MSE={avg_epoch_val_mse:.4f}, RMSE={avg_epoch_val_rmse:.4f}")
            
            # --- æ—©åœé€»è¾‘åˆ¤æ–­ (MODIFIED) ---
            if avg_epoch_val_loss < best_val_loss:
                best_val_loss = avg_epoch_val_loss
                torch.save({ 'epoch': epoch, 'model_state_dict': model.state_dict(),
                             'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss,
                             'train_losses_history': train_losses_history, 'val_losses_history': val_losses_history,
                             'train_mses_history': train_mses_history, 'train_rmses_history': train_rmses_history,
                             'val_mses_history': val_mses_history, 'val_rmses_history': val_rmses_history
                             # å¯é€‰: 'early_stopping_counter': early_stopping_counter
                           }, best_model_path)
                tqdm.write(f"âœ… U-Netæœ€ä½³æ¨¡å‹å·²ä¿å­˜, éªŒè¯Loss: {best_val_loss:.6f}")
                early_stopping_counter = 0 # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                early_stopping_counter += 1 # éªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„
                tqdm.write(f"ğŸŸ¡ æ—©åœè®¡æ•°å™¨: {early_stopping_counter}/{early_stopping_patience} (å½“å‰éªŒè¯Loss: {avg_epoch_val_loss:.6f}, æœ€ä½³: {best_val_loss:.6f})")
            # --- å·²ç§»é™¤æ‚¨ä¹‹å‰ç‰ˆæœ¬ä¸­åœ¨æ­¤å¤„é”™è¯¯æ·»åŠ çš„ val_losses_history.append(float('nan')) ---
            if progress_bar_val: progress_bar_val.close() # å…³é—­éªŒè¯è¿›åº¦æ¡
        else: # å¤„ç†æ²¡æœ‰ test_loader çš„æƒ…å†µ (è¿™éƒ¨åˆ†åœ¨æ‚¨çš„ä»£ç ä¸­å·²ç»æ˜¯æ­£ç¡®çš„)
            val_losses_history.append(float('nan')); val_mses_history.append(float('nan')); val_rmses_history.append(float('nan'))
        
        progress_bar_train.close() 
        # if test_loader and 'progress_bar_val' in locals() and progress_bar_val: progress_bar_val.close() # å·²ç§»åˆ° if test_loader å†…éƒ¨

        torch.save({ 'epoch': epoch, 'model_state_dict': model.state_dict(),
                     'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss,
                     'train_losses_history': train_losses_history, 'val_losses_history': val_losses_history,
                     'train_mses_history': train_mses_history, 'train_rmses_history': train_rmses_history,
                     'val_mses_history': val_mses_history, 'val_rmses_history': val_rmses_history
                     # å¯é€‰: 'early_stopping_counter': early_stopping_counter
                   }, latest_model_path)

        with open(log_path, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([epoch + 1, avg_epoch_train_loss, avg_epoch_train_mse, avg_epoch_train_rmse,
                             f"{avg_epoch_val_loss:.4f}" if not math.isnan(avg_epoch_val_loss) else "N/A",
                             f"{avg_epoch_val_mse:.4f}" if not math.isnan(avg_epoch_val_mse) else "N/A",
                             f"{avg_epoch_val_rmse:.4f}" if not math.isnan(avg_epoch_val_rmse) else "N/A"])
        
        # --- æ—©åœæ£€æŸ¥ (è¿™éƒ¨åˆ†åœ¨æ‚¨çš„ä»£ç ä¸­å·²ç»æ˜¯æ­£ç¡®çš„) ---
        if test_loader and early_stopping_counter >= early_stopping_patience:
            tqdm.write(f"ğŸ”´ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±å·²è¿ç»­ {early_stopping_patience} ä¸ªè½®æ¬¡æ²¡æœ‰æ”¹å–„ã€‚åœ¨ Epoch {epoch + 1} åœæ­¢è®­ç»ƒã€‚")
            break 
            
    tqdm.write('ğŸ U-Netè®­ç»ƒå®Œæˆã€‚')

    plt.figure(figsize=(18, 6))
    plt.subplot(1, 3, 1); plt.plot(train_losses_history, label='è®­ç»ƒæŸå¤±'); plt.plot(val_losses_history, label='éªŒè¯æŸå¤±', linestyle='--'); plt.xlabel('è½®æ¬¡'); plt.ylabel('æŸå¤±'); plt.legend(); plt.title('U-NetæŸå¤±'); plt.grid(True)
    plt.subplot(1, 3, 2); plt.plot(train_mses_history, label='è®­ç»ƒMSE'); plt.plot(val_mses_history, label='éªŒè¯MSE', linestyle='--'); plt.xlabel('è½®æ¬¡'); plt.ylabel('MSE'); plt.legend(); plt.title('U-Net MSE'); plt.grid(True)
    plt.subplot(1, 3, 3); plt.plot(train_rmses_history, label='è®­ç»ƒRMSE'); plt.plot(val_rmses_history, label='éªŒè¯RMSE', linestyle='--'); plt.xlabel('è½®æ¬¡'); plt.ylabel('RMSE'); plt.legend(); plt.title('U-Net RMSE'); plt.grid(True)
    plt.tight_layout(); plt.savefig(os.path.join(checkpoint_dir, "training_metrics_unet_norm_zh.png"))
    print(f"ğŸ“ˆ U-Netè®­ç»ƒæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜åˆ° {os.path.join(checkpoint_dir, 'training_metrics_unet_norm_zh.png')}")