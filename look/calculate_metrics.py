import os
import torch
import numpy as np
import pandas as pd
import cv2
import math
from tqdm import tqdm
from natsort import natsorted
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
import glob # ç¡®ä¿åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥globåº“

# --- 1. ç›¸å¯¹è·¯å¾„å¯¼å…¥æ¨¡å‹å®šä¹‰ ---
# å‡è®¾æ­¤è„šæœ¬ä¸ COMPARE å’Œ REMOTE æ–‡ä»¶å¤¹åœ¨åŒä¸€çº§ç›®å½•ä¸‹
print("æ­£åœ¨ä» COMPARE ç›®å½•å¯¼å…¥æ¨¡å‹å®šä¹‰...")
try:
    from COMPARE.unet import UNet
    from COMPARE.FCN import FCN_ResNet50_Regression
    from COMPARE.segnet import SegNet
    from COMPARE.PSPNET import PSPNet
    from COMPARE.deeplab import DeepLabV3ForRegression
    from COMPARE.CDLmix import ResnetWithComplexNet
    from COMPARE.CDL_ASPP import DeepLabV3WithComplexNet
    # âš ï¸ è¯·ç¡®ä¿ two_channelCDL.py æ–‡ä»¶ä¸­å®šä¹‰çš„ç±»åæ˜¯ DeepLabV3WithComplexNet2channeel
    from COMPARE.two_channelCDL import DeepLabV3WithComplexNet2channeel
    print("âœ… æ‰€æœ‰æ¨¡å‹å¯¼å…¥æˆåŠŸï¼")
except ImportError as e:
    print(f"âŒ æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ­¤è„šæœ¬ä½äºé¡¹ç›®æ ¹ç›®å½•ï¼Œä¸” COMPARE æ–‡ä»¶å¤¹åŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨¡å‹ .py æ–‡ä»¶ã€‚")
    exit()


# --- 2. é…ç½®åŒºåŸŸ ---
# âš ï¸ æ•°æ®æ ¹ç›®å½•ï¼Œè¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹
DATA_ROOT_DIR = "E:/EMTdata" 
# âš ï¸ æ¨¡å‹æƒé‡æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½• (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
BASE_MODELS_DIR = "." 

# å®šä¹‰éœ€è¦è¯„ä¼°çš„æ¨¡å‹
# é”®ååº”ä¸ REMOTE æ–‡ä»¶å¤¹ä¸‹çš„å­æ–‡ä»¶å¤¹å‰ç¼€å®Œå…¨ä¸€è‡´
MODEL_CLASS_MAP = {
    'DeepLabV3_Reg_UnifiedData': DeepLabV3ForRegression,
    'ComplexNetDeepLab_Reg': ResnetWithComplexNet,
    'ComplexNetDeepLab+aspp_Reg': DeepLabV3WithComplexNet,
    # é”®å '6vComplexNetDeepLab_2Channel_Reg' æ¥è‡ªæ‚¨çš„æˆªå›¾ï¼Œè¯·ç¡®ä¿å®ƒä¸æ–‡ä»¶å¤¹ååŒ¹é…
    '6vComplexNetDeepLab_2Channel_Reg': DeepLabV3WithComplexNet2channeel 
    # å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–æ¨¡å‹ï¼Œè¯·åœ¨è¿™é‡Œæ·»åŠ ï¼Œä¾‹å¦‚:
    # 'UNet_XLSX_Regression_Normalized': lambda: UNet(n_channels_in=3, n_channels_out=2),
    # 'FCN_XLSX_Regression': FCN_ResNet50_Regression,
}

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 4 # ä½¿ç”¨ç¨å¤§çš„batch sizeå¯ä»¥åŠ é€Ÿè®¡ç®—

# --- 3. çµæ´»çš„æ•°æ®é›†å®šä¹‰ (å¯å¤„ç†2é€šé“æˆ–3é€šé“) ---
def normalize_f_data(data, min_val=2.0, max_val=8.0):
    data = data.astype(np.float32)
    return (data - min_val) / (max_val - min_val) if (max_val - min_val) != 0 else data - min_val

class MatrixDataset(Dataset):
    def __init__(self, re_paths, im_paths, f_paths, label_re_paths, label_im_paths,
                 re_mean, re_std, im_mean, im_std, include_f_channel=True):
        self.re_paths, self.im_paths, self.f_paths = re_paths, im_paths, f_paths
        self.label_re_paths, self.label_im_paths = label_re_paths, label_im_paths
        self.re_mean, self.re_std = re_mean, re_std
        self.im_mean, self.im_std = im_mean, im_std
        self.include_f_channel = include_f_channel
        self.target_size = (512, 512)

    def __len__(self):
        return len(self.re_paths)

    def __getitem__(self, idx):
        re_data = pd.read_excel(self.re_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        im_data = pd.read_excel(self.im_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        
        re_data_resized = cv2.resize(re_data, self.target_size, interpolation=cv2.INTER_LINEAR)
        im_data_resized = cv2.resize(im_data, self.target_size, interpolation=cv2.INTER_LINEAR)

        # æ ‡å‡†åŒ–
        if self.re_std > 1e-7: re_data_resized = (re_data_resized - self.re_mean) / self.re_std
        if self.im_std > 1e-7: im_data_resized = (im_data_resized - self.im_mean) / self.im_std

        channels = [re_data_resized, im_data_resized]
        if self.include_f_channel:
            f_data_orig = pd.read_excel(self.f_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
            f_data_resized = cv2.resize(normalize_f_data(f_data_orig), self.target_size, interpolation=cv2.INTER_LINEAR)
            channels.append(f_data_resized)
            
        input_tensor = torch.from_numpy(np.stack(channels, axis=0)).float()

        label_re_orig = pd.read_csv(self.label_re_paths[idx], header=None).values.astype(np.float32)
        label_im_orig = pd.read_csv(self.label_im_paths[idx], header=None).values.astype(np.float32)
        label_re_resized = cv2.resize(label_re_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_im_resized = cv2.resize(label_im_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_tensor = torch.from_numpy(np.stack([label_re_resized, label_im_resized], axis=0)).float()

        return input_tensor, label_tensor

# --- 4. æ•°æ®åŠ è½½å‡½æ•° ---
def get_test_dataloader(data_folder, batch_size, include_f_channel=True):
    print(f"æ­£åœ¨æ„å»º{'3é€šé“' if include_f_channel else '2é€šé“'}æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    # æ„å»ºè·¯å¾„
    base_input_path = os.path.join(data_folder, 'E') if os.path.exists(os.path.join(data_folder, 'E')) else data_folder
    base_label_path = os.path.join(data_folder, 'label') if os.path.exists(os.path.join(data_folder, 'label')) else data_folder
    
    re_folder_path = os.path.join(base_input_path, 'Re')
    im_folder_path = os.path.join(base_input_path, 'Im')
    f_folder_path = os.path.join(base_input_path, 'F')
    label_re_folder_path = os.path.join(base_label_path, 'label_Re')
    label_im_folder_path = os.path.join(base_label_path, 'label_Im')
    
    # è·å–æ–‡ä»¶å
    re_files = natsorted(os.listdir(re_folder_path))
    im_files = natsorted(os.listdir(im_folder_path))
    f_files = natsorted(os.listdir(f_folder_path)) if include_f_channel else []
    label_re_files = natsorted(os.listdir(label_re_folder_path))
    label_im_files = natsorted(os.listdir(label_im_folder_path))

    # åŒæ­¥æ–‡ä»¶åˆ—è¡¨
    file_lists = [re_files, im_files, label_re_files, label_im_files]
    if include_f_channel:
        file_lists.insert(2, f_files)
    min_len = min(len(lst) for lst in file_lists)
    
    file_tuples = list(zip(*(lst[:min_len] for lst in file_lists)))
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ä»¥ç¡®å®šæµ‹è¯•æ–‡ä»¶å’Œæ ‡å‡†åŒ–å‚æ•°
    train_files, test_files = train_test_split(file_tuples, test_size=0.2, random_state=42)
    
    # è®¡ç®—è®­ç»ƒé›†çš„æ ‡å‡†åŒ–å‚æ•°
    print("è®¡ç®—æ ‡å‡†åŒ–å‚æ•°...")
    re_sum, im_sum, re_sum_sq, im_sum_sq, total_pixels = 0.0, 0.0, 0.0, 0.0, 0
    for files in tqdm(train_files, desc="Stat Calc"):
        re_data = pd.read_excel(os.path.join(re_folder_path, files[0]), header=0, engine='openpyxl').values
        im_data = pd.read_excel(os.path.join(im_folder_path, files[1]), header=0, engine='openpyxl').values
        re_sum += np.sum(re_data); im_sum += np.sum(im_data)
        re_sum_sq += np.sum(np.square(re_data)); im_sum_sq += np.sum(np.square(im_data))
        total_pixels += re_data.size
    
    re_mean = re_sum / total_pixels
    im_mean = im_sum / total_pixels
    re_std = math.sqrt(max(0, (re_sum_sq / total_pixels) - (re_mean ** 2)))
    im_std = math.sqrt(max(0, (im_sum_sq / total_pixels) - (im_mean ** 2)))
    print(f"ç»Ÿè®¡å®Œæˆ: Re(Î¼={re_mean:.4f}, Ïƒ={re_std:.4f}), Im(Î¼={im_mean:.4f}, Ïƒ={im_std:.4f})")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    if include_f_channel:
        test_dataset = MatrixDataset(
            re_paths=[os.path.join(re_folder_path, f[0]) for f in test_files],
            im_paths=[os.path.join(im_folder_path, f[1]) for f in test_files],
            f_paths=[os.path.join(f_folder_path, f[2]) for f in test_files],
            label_re_paths=[os.path.join(label_re_folder_path, f[3]) for f in test_files],
            label_im_paths=[os.path.join(label_im_folder_path, f[4]) for f in test_files],
            re_mean=re_mean, re_std=re_std, im_mean=im_mean, im_std=im_std,
            include_f_channel=True
        )
    else: # 2é€šé“
        test_dataset = MatrixDataset(
            re_paths=[os.path.join(re_folder_path, f[0]) for f in test_files],
            im_paths=[os.path.join(im_folder_path, f[1]) for f in test_files],
            f_paths=[],
            label_re_paths=[os.path.join(label_re_folder_path, f[2]) for f in test_files],
            label_im_paths=[os.path.join(label_im_folder_path, f[3]) for f in test_files],
            re_mean=re_mean, re_std=re_std, im_mean=im_mean, im_std=im_std,
            include_f_channel=False
        )
        
    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)


# --- 5. æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—å‡½æ•° ---
def calculate_all_metrics(model, dataloader, device):
    model.to(device)
    model.eval()
    
    all_psnr, all_ssim, all_mse, all_rmse = [], [], [], []

    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc="æ­£åœ¨è¯„ä¼°"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)

            # å°†Tensorè½¬ä¸ºNumpyæ•°ç»„ä»¥ä¾¿è®¡ç®—
            outputs_np = outputs.cpu().numpy()
            labels_np = labels.cpu().numpy()

            for i in range(outputs_np.shape[0]): # éå†batchä¸­çš„æ¯ä¸ªæ ·æœ¬
                true_img, pred_img = labels_np[i], outputs_np[i]
                
                # è®¡ç®— MSE, RMSE
                mse = np.mean((true_img - pred_img) ** 2)
                all_mse.append(mse)
                all_rmse.append(np.sqrt(mse))

                # è®¡ç®— PSNR, SSIM
                # data_range æ˜¯å…³é”®å‚æ•°ï¼Œä»£è¡¨å›¾åƒçš„åŠ¨æ€èŒƒå›´
                # ç”±äºæ ‡ç­¾ä¸æ˜¯å½’ä¸€åŒ–åˆ°[0,1]çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨å®é™…çš„èŒƒå›´
                data_range = true_img.max() - true_img.min()
                if data_range == 0: continue # é¿å…é™¤ä»¥é›¶

                # å¯¹å®éƒ¨å’Œè™šéƒ¨åˆ†åˆ«è®¡ç®—ï¼Œç„¶åæ±‚å¹³å‡
                psnr_re = psnr(true_img[0], pred_img[0], data_range=data_range)
                psnr_im = psnr(true_img[1], pred_img[1], data_range=data_range)
                all_psnr.append((psnr_re + psnr_im) / 2)

                ssim_re = ssim(true_img[0], pred_img[0], data_range=data_range)
                ssim_im = ssim(true_img[1], pred_img[1], data_range=data_range)
                all_ssim.append((ssim_re + ssim_im) / 2)

    return {
        "MSE": np.mean(all_mse),
        "RMSE": np.mean(all_rmse),
        "PSNR": np.mean(all_psnr),
        "SSIM": np.mean(all_ssim)
    }

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ¨¡å‹æ€§èƒ½è¯„ä¼°è„šæœ¬...")
    print(f"å°†ä½¿ç”¨è®¾å¤‡: {DEVICE}")

    # 1. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    loader_3_channel = get_test_dataloader(DATA_ROOT_DIR, BATCH_SIZE, include_f_channel=True)
    loader_2_channel = get_test_dataloader(DATA_ROOT_DIR, BATCH_SIZE, include_f_channel=False)
    
    results = {}

    # 2. éå†æ‰€æœ‰æ¨¡å‹è¿›è¡Œè¯„ä¼°
    for model_key, model_class in MODEL_CLASS_MAP.items():
        print(f"\n{'='*25}\næ­£åœ¨å¤„ç†æ¨¡å‹: {model_key}\n{'='*25}")
        
        # å®ä¾‹åŒ–æ¨¡å‹
        if "UNet" in model_key:
             model = model_class()
        else:
             model = model_class()

        # æ ¹æ®æ¨¡å‹åé€‰æ‹©å¯¹åº”çš„æ•°æ®åŠ è½½å™¨
        if "2Channel" in model_key:
            current_loader = loader_2_channel
            print("INFO: æ£€æµ‹åˆ° '2Channel' å…³é”®å­—ï¼Œä½¿ç”¨2é€šé“æ•°æ®åŠ è½½å™¨ã€‚")
        else:
            current_loader = loader_3_channel
            print("INFO: ä½¿ç”¨é»˜è®¤çš„3é€šé“æ•°æ®åŠ è½½å™¨ã€‚")

        # --- âœ¨âœ¨âœ¨ æœ€ç»ˆä¿®æ­£ç‚¹ï¼šä½¿ç”¨ glob åŠ¨æ€æŸ¥æ‰¾æ–‡ä»¶ âœ¨âœ¨âœ¨ ---
        
        # 1. å®šä¹‰æ¨¡å‹çš„ä¸»è¾“å‡ºæ–‡ä»¶å¤¹å
        model_folder_name = f"{model_key}_Output"
        if model_key == '6vComplexNetDeepLab_2Channel_Reg':
             model_folder_name = "6vComplexNetDeepLab_2Channel_Reg_Output"
        
        # 2. æ„å»ºæ¨¡å‹è¾“å‡ºçš„æ ¹ç›®å½•
        model_output_root_dir = os.path.join(BASE_MODELS_DIR, model_folder_name)

        # 3. ä½¿ç”¨ glob é€’å½’æœç´¢ *_best_model.pth æ–‡ä»¶
        # `**` è¡¨ç¤ºæœç´¢æ‰€æœ‰å­ç›®å½•
        search_pattern = os.path.join(model_output_root_dir, '**', '*_best_model.pth')
        print(f"æ­£åœ¨æœç´¢: {search_pattern}")
        pth_path_list = glob.glob(search_pattern, recursive=True)

        # 4. æ£€æŸ¥æœç´¢ç»“æœ
        if not pth_path_list:
            print(f"âš ï¸ è­¦å‘Š: åœ¨ç›®å½• {model_output_root_dir} åŠå…¶æ‰€æœ‰å­ç›®å½•ä¸­å‡æœªæ‰¾åˆ°åŒ¹é… '*_best_model.pth' çš„æ–‡ä»¶ã€‚")
            print("è¯·æ£€æŸ¥æ‚¨çš„ MODEL_CLASS_MAP é”®åæ˜¯å¦ä¸æ–‡ä»¶å¤¹åå®Œå…¨å¯¹åº”ã€‚è·³è¿‡æ­¤æ¨¡å‹ã€‚")
            continue
        
        # å¦‚æœæ‰¾åˆ°å¤šä¸ªï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
        pth_path = pth_path_list[0] 
        # --- ä¿®æ­£ç»“æŸ ---

        try:
            print(f"âœ… æ‰¾åˆ°æƒé‡æ–‡ä»¶: {pth_path}")
            print(f"æ­£åœ¨åŠ è½½...")
            checkpoint = torch.load(pth_path, map_location=DEVICE)
            
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            print("æƒé‡åŠ è½½æˆåŠŸã€‚")

            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_all_metrics(model, current_loader, DEVICE)
            results[model_key] = metrics

        except Exception as e:
            print(f"âŒ å¤„ç†æ¨¡å‹ {model_key} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    # 3. æ‰“å°æœ€ç»ˆçš„æ€»ç»“è¡¨æ ¼
    print("\n\n" + "="*60)
    print("ğŸ“Š æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*60)
    
    if not results:
        print("æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ¨¡å‹ã€‚è¯·æ£€æŸ¥æ¨¡å‹é”®åå’Œæ–‡ä»¶å¤¹è·¯å¾„æ˜¯å¦å®Œå…¨å¯¹åº”ã€‚")
    else:
        header = f"{'Model':<40} | {'MSE':<10} | {'RMSE':<10} | {'PSNR':<10} | {'SSIM':<10}"
        print(header)
        print("-" * len(header))
        for model_name, metrics in results.items():
            print(f"{model_name:<40} | {metrics['MSE']:<10.4f} | {metrics['RMSE']:<10.4f} | {metrics['PSNR']:<10.4f} | {metrics['SSIM']:<10.4f}")
    
    print("="*60)
    print("ğŸ è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
