# æ–‡ä»¶å: calculate_final_metrics.py

import os
import glob
import torch
import pandas as pd
import numpy as np
import cv2
import math
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from natsort import natsorted
from tqdm import tqdm

# å¯¼å…¥æˆ‘ä¹‹å‰ç»™ä½ çš„æŒ‡æ ‡è®¡ç®—å™¨
from torchmetrics import PeakSignalNoiseRatio
from torchmetrics.image import StructuralSimilarityIndexMeasure

# âš ï¸ 1. ã€ä»ä½ çš„ä»£ç ä¸­å¤åˆ¶ã€‘å¯¼å…¥ä½ å®šä¹‰çš„æ‰€æœ‰æ¨¡å‹ç±»
# ==============================================================================
from COMPARE.unet import UNet
from COMPARE.FCN import FCN_ResNet50_Regression
from COMPARE.segnet import SegNet
from COMPARE.PSPNET import PSPNet
from COMPARE.deeplab import DeepLabV3ForRegression
from COMPARE.CDLmix import ResnetWithComplexNet
from COMPARE.CDL_ASPP import DeepLabV3WithComplexNet
# ==============================================================================

# âš ï¸ 2. ã€ä»ä½ çš„ä»£ç ä¸­å¤åˆ¶ã€‘ä½ çš„æ•°æ®åŠ è½½å‡½æ•° (æ— éœ€ä»»ä½•ä¿®æ”¹)
# ==============================================================================
def normalize_f_data(data, min_val=2.0, max_val=8.0):
    data = data.astype(np.float32)
    if (max_val - min_val) == 0: return data - min_val
    return (data - min_val) / (max_val - min_val)

class MatrixDataset(Dataset):
    def __init__(self, re_paths, im_paths, f_paths, label_re_paths, label_im_paths,
                 re_mean=None, re_std=None, im_mean=None, im_std=None, calculate_stats_mode=False):
        self.re_paths, self.im_paths, self.f_paths = re_paths, im_paths, f_paths
        self.label_re_paths = label_re_paths if label_re_paths else []
        self.label_im_paths = label_im_paths if label_im_paths else []
        self.target_size = (512, 512)
        self.re_mean, self.re_std, self.im_mean, self.im_std = re_mean, re_std, im_mean, im_std
        self.calculate_stats_mode = calculate_stats_mode

    def __len__(self):
        return len(self.re_paths)

    def __getitem__(self, idx):
        re_data = pd.read_excel(self.re_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        im_data = pd.read_excel(self.im_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        f_data_orig = pd.read_excel(self.f_paths[idx], header=0, engine='openpyxl').values.astype(np.float32)
        
        re_data_resized = cv2.resize(re_data, self.target_size, interpolation=cv2.INTER_LINEAR)
        im_data_resized = cv2.resize(im_data, self.target_size, interpolation=cv2.INTER_LINEAR)
        f_data_resized = cv2.resize(normalize_f_data(f_data_orig), self.target_size, interpolation=cv2.INTER_LINEAR)

        if not self.calculate_stats_mode:
            if self.re_mean is not None and self.re_std is not None and self.re_std > 1e-7:
                re_data_resized = (re_data_resized - self.re_mean) / self.re_std
            if self.im_mean is not None and self.im_std is not None and self.im_std > 1e-7:
                im_data_resized = (im_data_resized - self.im_mean) / self.im_std
        
        input_data_np = np.stack([re_data_resized, im_data_resized, f_data_resized], axis=0)
        input_tensor = torch.from_numpy(input_data_np).float()

        if self.calculate_stats_mode:
            return input_tensor

        label_re_orig = pd.read_csv(self.label_re_paths[idx], header=None).values.astype(np.float32)
        label_im_orig = pd.read_csv(self.label_im_paths[idx], header=None).values.astype(np.float32)
        label_re_resized = cv2.resize(label_re_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_im_resized = cv2.resize(label_im_orig, self.target_size, interpolation=cv2.INTER_LINEAR)
        label_data_np = np.stack([label_re_resized, label_im_resized], axis=0)
        label_tensor = torch.from_numpy(label_data_np).float()
        return input_tensor, label_tensor

def get_test_dataloader(data_folder, test_split_ratio=0.2, random_state=42, batch_size=1):
    # (æ­¤å‡½æ•°ä¸ä½ æä¾›çš„å®Œå…¨ä¸€è‡´ï¼Œè¿™é‡Œçœç•¥ä»¥èŠ‚çœç©ºé—´ï¼Œå®é™…ä½¿ç”¨æ—¶è¯·å®Œæ•´å¤åˆ¶)
    # ... å®Œæ•´å¤åˆ¶ä½ ä»£ç ä¸­çš„ get_test_dataloader å‡½æ•° ...
    print("ğŸš€ å¼€å§‹æ„å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    print("ğŸ” æ­£åœ¨è§£ææ–‡ä»¶è·¯å¾„...")
    E_folder_path_check = os.path.join(data_folder, 'E')
    if os.path.exists(E_folder_path_check):
      f_folder_path, re_folder_path, im_folder_path = [os.path.join(E_folder_path_check, d) for d in ['F', 'Re', 'Im']]
    else:
      f_folder_path, re_folder_path, im_folder_path = [os.path.join(data_folder, d) for d in ['F', 'Re', 'Im']]
    label_folder_path_check = os.path.join(data_folder, 'label')
    if os.path.exists(label_folder_path_check):
      label_re_folder_path, label_im_folder_path = [os.path.join(label_folder_path_check, d) for d in ['label_Re', 'label_Im']]
    else:
      label_re_folder_path, label_im_folder_path = [os.path.join(data_folder, d) for d in ['label_Re', 'label_Im']]
    print("è·¯å¾„è§£æå®Œæ¯•ã€‚")
    print("ğŸ§¾ æ­£åœ¨åˆ—å‡ºå¹¶å¯¹é½æ‰€æœ‰æ•°æ®æ–‡ä»¶...")
    re_all_filenames, im_all_filenames, f_all_filenames = [natsorted(os.listdir(p)) for p in [re_folder_path, im_folder_path, f_folder_path]]
    label_re_all_filenames, label_im_all_filenames = [natsorted(os.listdir(p)) for p in [label_re_folder_path, label_im_folder_path]]
    min_count = min(len(f) for f in [re_all_filenames, im_all_filenames, f_all_filenames, label_re_all_filenames, label_im_all_filenames])
    re_f, im_f, f_f, lr_f, li_f = [lst[:min_count] for lst in [re_all_filenames, im_all_filenames, f_all_filenames, label_re_all_filenames, label_im_all_filenames]]
    print(f"æ–‡ä»¶å¯¹é½å®Œæ¯•ï¼Œå…±æ‰¾åˆ° {min_count} ç»„åŒ¹é…æ•°æ®ã€‚")
    print(f"ğŸ”ª æ­£åœ¨ä»¥ random_state={random_state} åˆ’åˆ†æ•°æ®é›†...")
    train_files, test_files = train_test_split(list(zip(re_f, im_f, f_f, lr_f, li_f)), test_size=test_split_ratio, random_state=random_state)
    re_train_files, im_train_files, f_train_files, _, _ = zip(*train_files)
    re_test_files, im_test_files, f_test_files, label_re_test_files, label_im_test_files = zip(*test_files)
    print(f"åˆ’åˆ†å®Œæ¯•: è®­ç»ƒé›† {len(train_files)} ä¸ª, æµ‹è¯•é›† {len(test_files)} ä¸ªã€‚")
    print("ğŸ“Š æ­£åœ¨è®¡ç®—ã€è®­ç»ƒé›†ã€‘çš„å‡å€¼å’Œæ ‡å‡†å·®ç”¨äºå½’ä¸€åŒ–...")
    stat_dataset = MatrixDataset(re_paths=[os.path.join(re_folder_path, f) for f in re_train_files], im_paths=[os.path.join(im_folder_path, f) for f in im_train_files], f_paths=[os.path.join(f_folder_path, f) for f in f_train_files], label_re_paths=[], label_im_paths=[], calculate_stats_mode=True)
    stat_loader = DataLoader(stat_dataset, batch_size=8, shuffle=False, num_workers=0)
    re_sum, im_sum, re_sum_sq, im_sum_sq, total_pixels = 0.0, 0.0, 0.0, 0.0, 0
    for inputs in tqdm(stat_loader, desc="è®¡ç®—ç»Ÿè®¡é‡"):
        re_ch, im_ch = inputs[:, 0, :, :], inputs[:, 1, :, :]
        re_sum += torch.sum(re_ch).item(); im_sum += torch.sum(im_ch).item()
        re_sum_sq += torch.sum(torch.square(re_ch)).item(); im_sum_sq += torch.sum(torch.square(im_ch)).item()
        total_pixels += re_ch.nelement()
    re_m_train, im_m_train = re_sum / total_pixels, im_sum / total_pixels
    re_s_train, im_s_train = math.sqrt(max(0, (re_sum_sq/total_pixels) - (re_m_train**2))), math.sqrt(max(0, (im_sum_sq/total_pixels) - (im_m_train**2)))
    print(f"ç»Ÿè®¡ç»“æœ: Re(å‡å€¼={re_m_train:.4f}, Std={re_s_train:.4f}), Im(å‡å€¼={im_m_train:.4f}, Std={im_s_train:.4f})")
    print("ğŸ“¦ æ­£åœ¨åˆ›å»ºæœ€ç»ˆçš„æµ‹è¯•é›†åŠ è½½å™¨...")
    test_dataset = MatrixDataset(re_paths=[os.path.join(re_folder_path, f) for f in re_test_files], im_paths=[os.path.join(im_folder_path, f) for f in im_test_files], f_paths=[os.path.join(f_folder_path, f) for f in f_test_files], label_re_paths=[os.path.join(label_re_folder_path, f) for f in label_re_test_files], label_im_paths=[os.path.join(label_im_folder_path, f) for f in label_im_test_files], re_mean=re_m_train, re_std=re_s_train, im_mean=im_m_train, im_std=im_s_train)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬ã€‚")
    return test_loader
# ==============================================================================

# âš ï¸ 3. ã€è¿™æ˜¯æˆ‘ä»¬çš„æ–°æ ¸å¿ƒå‡½æ•°ã€‘ç”¨äºè¯„ä¼°æ‰€æœ‰æ¨¡å‹
# ==============================================================================
def calculate_metrics_for_all_models(base_dir, model_map, test_loader, device, data_range=59.0220):
    """
    åŠ è½½æ¯ä¸ªæ¨¡å‹çš„æœ€ä½³æƒé‡ï¼Œåœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè®¡ç®—PSNRå’ŒSSIMï¼Œå¹¶è¿”å›ç»“æœã€‚
    """
    print("\n" + "="*80)
    print("ğŸš€ Running Final Metric Calculation for All Models...")
    print("="*80)
    
    all_results = []

    # éå†ä½ åœ¨ä¸»å‡½æ•°é‡Œå®šä¹‰çš„æ¯ä¸€ä¸ªæ¨¡å‹
    for model_name_key, model_class in model_map.items():
        folder_name = model_name_key + "_Output"
        folder_path = os.path.join(base_dir, folder_name)
        
        try:
            # --- æ¨¡å‹åŠ è½½é€»è¾‘ (å®Œå…¨å¤ç”¨ä½ çš„ä»£ç ) ---
            # 1. å®ä¾‹åŒ–æ¨¡å‹ï¼Œè¿™é‡Œè¦ç‰¹åˆ«å¤„ç†lambdaå‡½æ•°
            # callable()å¯ä»¥åŒæ—¶åˆ¤æ–­æ™®é€šå‡½æ•°å’Œlambdaå‡½æ•°
            if callable(model_class) and not isinstance(model_class, type):
                model = model_class() # è°ƒç”¨lambda: UNet(...)
            else:
                model = model_class() # ç›´æ¥å®ä¾‹åŒ– MyModel()
                
            # 2. æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
            pth_path_list = glob.glob(os.path.join(folder_path, '*_best_model.pth'))
            if not pth_path_list:
                print(f"âš ï¸ Skipping {model_name_key}: No '*_best_model.pth' found in {folder_path}")
                continue
            
            # 3. åŠ è½½æƒé‡
            pth_path = pth_path_list[0]
            checkpoint = torch.load(pth_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            print(f"\nâœ… Successfully loaded model: {model_name_key}")

            # --- æŒ‡æ ‡è®¡ç®—é€»è¾‘ (æ–°æ·»åŠ çš„æ ¸å¿ƒéƒ¨åˆ†) ---
            # 1. ä¸ºå½“å‰æ¨¡å‹åˆå§‹åŒ–æ–°çš„æŒ‡æ ‡è®¡ç®—å™¨
            psnr_metric = PeakSignalNoiseRatio(data_range=data_range).to(device)
            ssim_metric = StructuralSimilarityIndexMeasure(data_range=data_range).to(device)

            # 2. åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
            with torch.no_grad():
                for inputs, ground_truth in tqdm(test_loader, desc=f"Evaluating {model_name_key}"):
                    inputs = inputs.to(device)
                    ground_truth = ground_truth.to(device)
                    
                    predictions = model(inputs)
                    
                    # ç´¯ç§¯è®¡ç®—æŒ‡æ ‡
                    psnr_metric.update(predictions, ground_truth)
                    ssim_metric.update(predictions, ground_truth)

            # 3. è®¡ç®—æœ€ç»ˆå¹³å‡å€¼
            final_psnr = psnr_metric.compute().item() # .item()è·å–çº¯æ•°å€¼
            final_ssim = ssim_metric.compute().item()

            print(f"  - Final PSNR: {final_psnr:.4f}")
            print(f"  - Final SSIM: {final_ssim:.4f}")

            # 4. ä¿å­˜ç»“æœ
            all_results.append({
                'Model': model_name_key,
                'PSNR': final_psnr,
                'SSIM': final_ssim
            })

        except Exception as e:
            print(f"âŒ Failed to process model {model_name_key}: {e}")
            # raise e # å¦‚æœéœ€è¦è°ƒè¯•ï¼Œå¯ä»¥å–æ¶ˆè¿™è¡Œæ³¨é‡Šæ¥æŸ¥çœ‹è¯¦ç»†é”™è¯¯

    return all_results
# ==============================================================================


if __name__ == "__main__":
    # --- å…¨å±€é…ç½® (ä»ä½ çš„ä»£ç ä¸­å¤åˆ¶) ---
    DATA_ROOT_DIR = "E:/EMTdata"
    BASE_DIR = "E:/EMMESData/Remote/"
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {DEVICE}")

    # --- æ¨¡å‹æ˜ å°„ (ä»ä½ çš„ä»£ç ä¸­å¤åˆ¶) ---
    MODEL_CLASS_MAP = {
        'ComplexNetDeepLab+aspp_Reg': DeepLabV3WithComplexNet,
        'ComplexNetDeepLab_Reg': ResnetWithComplexNet,
        'UNet_XLSX_Regression_Normalized': lambda: UNet(n_channels_in=3, n_channels_out=2),
        'DeepLabV3_Reg_UnifiedData': DeepLabV3ForRegression,
        'FCN_XLSX_Regression': FCN_ResNet50_Regression,
        'SegNet_XLSX': lambda: SegNet(input_channels=3, output_channels=2),
        'PSPNet_Regression_Unified': PSPNet
    }
    
    # è¿‡æ»¤æ‰æœªæä¾›æ¨¡å‹ç±»çš„æ¡ç›®
    active_model_map = {k: v for k, v in MODEL_CLASS_MAP.items() if v is not None}
    
    if not active_model_map:
        print("\nâŒ MODEL_CLASS_MAP is not configured. Please edit the script.")
    else:
        try:
            # --- 1. è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨ ---
            test_loader = get_test_dataloader(
                data_folder=DATA_ROOT_DIR,
                batch_size=8, # å¯ä»¥é€‚å½“è°ƒå¤§batch_sizeä»¥åŠ é€Ÿè¯„ä¼°
                random_state=42 
            )
            
            # --- 2. è¿è¡Œè¯„ä¼°å‡½æ•° ---
            final_results = calculate_metrics_for_all_models(
                base_dir=BASE_DIR,
                model_map=active_model_map,
                test_loader=test_loader,
                device=DEVICE,
                data_range=59.0220 # å‡è®¾ä½ çš„labelä¹Ÿè¢«å½’ä¸€åŒ–åˆ°äº†[0,1]ä¹‹é—´ï¼Œå¦‚æœä¸æ˜¯è¯·ä¿®æ”¹
            )

            # --- 3. æ•´ç†å¹¶å±•ç¤ºç»“æœ ---
            if final_results:
                results_df = pd.DataFrame(final_results)
                results_df = results_df.sort_values(by='PSNR', ascending=False) # æŒ‰PSNRé™åºæ’åº

                print("\n\n" + "="*80)
                print("ğŸ†ğŸ†ğŸ† FINAL PERFORMANCE METRICS ğŸ†ğŸ†ğŸ†")
                print("="*80)
                print(results_df.to_string(index=False))
                
                # ä¿å­˜åˆ°CSVï¼Œæ–¹ä¾¿ä½ å¤åˆ¶åˆ°è®ºæ–‡é‡Œ
                output_filename = "final_metrics_psnr_ssim.csv"
                results_df.to_csv(output_filename, index=False)
                print(f"\nâœ… Results table saved to '{output_filename}'")

        except Exception as e:
            print(f"âŒ An error occurred during the process: {e}")